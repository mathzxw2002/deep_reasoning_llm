
# Finetune 

## Unsloth
Train your own model with Unsloth, an open-source framework for LLM fine-tuning and reinforcement learning.

At Unsloth, our mission is to make AI as accurate and accessible as possible. Train, run, evaluate and save gpt-oss, Llama, DeepSeek, TTS, Qwen, Mistral, Gemma LLMs 2x faster with 70% less VRAM.

Our docs will guide you through running & training your own model locally.

https://docs.unsloth.ai/

https://github.com/unslothai/unsloth

<img width="3194" height="992" alt="image" src="https://github.com/user-attachments/assets/77cc39b1-ffa2-4560-b2bc-211107be2952" />

https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning




# Inference 

## huggingface optimum
https://github.com/huggingface/optimum

üöÄ Accelerate inference and training of ü§ó Transformers, Diffusers, TIMM and Sentence Transformers with easy to use hardware optimization tools

Optimum is an extension of Transformers ü§ñ Diffusers üß® TIMM üñºÔ∏è and Sentence-Transformers ü§ó, providing a set of optimization tools and enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use.


## sglang
SGLang is a fast serving framework for large language models and vision language models.

https://github.com/sgl-project/sglang

<img width="2392" height="728" alt="image" src="https://github.com/user-attachments/assets/22c6a945-370d-4822-a1d9-56b2431b5832" />

<img width="1281" height="651" alt="image" src="https://github.com/user-attachments/assets/b70fa506-df78-407f-846e-cf9297dbf4a3" />

## TensorRT-LLM
https://github.com/NVIDIA/TensorRT-LLM

TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.

<img width="1298" height="551" alt="image" src="https://github.com/user-attachments/assets/d3f8034e-562e-4d35-98cd-70a49532a0ad" />


## vllm
https://github.com/vllm-project/vllm

A high-throughput and memory-efficient inference and serving engine for LLMs

<img width="3000" height="856" alt="image" src="https://github.com/user-attachments/assets/55747f16-d07a-47ed-8ccd-43f07d1a4349" />

<img width="1292" height="1015" alt="image" src="https://github.com/user-attachments/assets/930f8167-a26b-40a8-85a7-c2d63f07b2a6" />



# LLM Quantization | GPTQ | QAT | AWQ | GGUF | GGML | PTQ

<img width="878" height="430" alt="image" src="https://github.com/user-attachments/assets/977f6bd6-58a3-49d4-9ac1-dc7a244463be" />


https://medium.com/@siddharth.vij10/llm-quantization-gptq-qat-awq-gguf-ggml-ptq-2e172cd1b3b5


## bitsandbytes



## huggingface optimum-quanto
A pytorch quantization backend for optimum

https://github.com/huggingface/optimum-quanto

<img width="1289" height="974" alt="image" src="https://github.com/user-attachments/assets/09e92133-d062-4f7a-b566-afb6efb2dcbd" />



## RWKV: Reinventing RNNs for the Transformer Era
https://arxiv.org/pdf/2305.13048

https://github.com/RWKV/rwkv.cpp


<img width="846" height="1090" alt="image" src="https://github.com/user-attachments/assets/325f637e-91bd-4a7c-a91b-4d4e10edc037" />


## Quantization formats

https://bentoml.com/llm/getting-started/llm-quantization

<img width="1224" height="755" alt="image" src="https://github.com/user-attachments/assets/45ab49d8-cce2-4c39-8c57-41b8913fda92" />


## AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

<img width="2016" height="462" alt="image" src="https://github.com/user-attachments/assets/9601966e-0180-4388-bf2b-68d93b75d06e" />


https://github.com/mit-han-lab/llm-awq


## 
