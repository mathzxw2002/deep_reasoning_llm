# deep_reasoning_llm

commands of docker 
https://www.runoob.com/docker/docker-command-manual.html


# TODO
- [ ] Llama Nemotron VLM Dataset V1:( https://huggingface.co/blog/nvidia/nvidia-vlm-dataset-v1), Model: https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1

# LLM Training Framework

## LLaMA-Factory
https://github.com/hiyouga/LLaMA-Factory
<img width="1400" height="400" alt="image" src="https://github.com/user-attachments/assets/d07b207b-7882-4de7-a376-f5d155d244a8" />

## ms-swift
https://github.com/modelscope/ms-swift
<img width="2048" height="544" alt="image" src="https://github.com/user-attachments/assets/2533eade-9d7f-4253-b8a9-62baec8060b4" />

## unsloth

<img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/dbdd0369-fee3-4a23-ac7b-a56e836c6be2" />

https://unsloth.ai/

## trl
https://huggingface.co/docs/trl/index

<img width="363" height="90" alt="image" src="https://github.com/user-attachments/assets/3192f1c3-fa76-4c98-917a-ba69563799e8" />

Fine-Tuning a Vision Language Model (Qwen2-VL-7B) with the Hugging Face Ecosystem (TRL)

https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl


https://huggingface.co/blog/vllm-colocate

## transformers
<img width="2240" height="780" alt="image" src="https://github.com/user-attachments/assets/75f1917b-3f70-4bb5-9f31-89d5648c5cbc" />

https://github.com/huggingface/transformers

https://huggingface.co/blog/smollm3

https://huggingface.co/blog/smolvlm2

https://github.com/huggingface/smollm

https://github.com/merveenoyan/smol-vision

# RAG


# LLM Inference Framework

## Vllm

## ollama

## https://github.com/NVIDIA/TensorRT-LLM

## optimum onnx
https://github.com/huggingface/optimum


# MCP

## Model compression toolkit

https://github.com/Tencent/AngelSlim



