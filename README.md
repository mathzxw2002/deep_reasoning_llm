# deep_reasoning_llm

commands of docker 
https://www.runoob.com/docker/docker-command-manual.html


# TODO

# LLM Training Framework

## LLaMA-Factory
https://github.com/hiyouga/LLaMA-Factory
<img width="1400" height="400" alt="image" src="https://github.com/user-attachments/assets/d07b207b-7882-4de7-a376-f5d155d244a8" />

## ms-swift
https://github.com/modelscope/ms-swift

## unsloth

<img width="1024" height="1024" alt="image" src="https://github.com/user-attachments/assets/dbdd0369-fee3-4a23-ac7b-a56e836c6be2" />

https://unsloth.ai/

## trl
https://huggingface.co/docs/trl/index

<img width="363" height="90" alt="image" src="https://github.com/user-attachments/assets/3192f1c3-fa76-4c98-917a-ba69563799e8" />

Fine-Tuning a Vision Language Model (Qwen2-VL-7B) with the Hugging Face Ecosystem (TRL)

https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl


https://huggingface.co/blog/vllm-colocate

## transformers
<img width="2240" height="780" alt="image" src="https://github.com/user-attachments/assets/75f1917b-3f70-4bb5-9f31-89d5648c5cbc" />

https://github.com/huggingface/transformers

https://huggingface.co/blog/smollm3

https://huggingface.co/blog/smolvlm2

https://github.com/huggingface/smollm

https://github.com/merveenoyan/smol-vision

# RAG


# LLM Inference Framework

## Vllm

## ollama

# MCP

## 

