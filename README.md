# deep_reasoning_llm

commands of docker 
https://www.runoob.com/docker/docker-command-manual.html

# Mu Li
https://github.com/mli/paper-reading?tab=readme-ov-file

# Flow Matching
https://neurips.cc/virtual/2024/tutorial/99531

# Training LLM on GPU Clusters

https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview

<img width="1338" height="927" alt="image" src="https://github.com/user-attachments/assets/5e3453cf-b9c9-4194-b50f-2eef8bca1209" />

https://www.youtube.com/watch?v=1E8GDR8QXKw

# Youtube of Sebastian Raschka
## LLM Building Blocks & Transformer Alternatives

https://www.youtube.com/watch?v=lONyteDR4XE

# TODO
- [ ] Llama Nemotron VLM Dataset V1:( https://huggingface.co/blog/nvidia/nvidia-vlm-dataset-v1), Model: https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1
- [ ] Multimodel Reasoning, e.g. GLM-4.5V
- [ ] Cosmos-Reason1 (https://github.com/nvidia-cosmos/cosmos-reason1) Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.
- [ ] Code w/ Claude Developer Conference (Claude Code https://www.youtube.com/playlist?list=PLf2m23nhTg1P5BsOHUOXyQz5RhfUSSVUi )
- [ ] https://github.com/ShaohonChen/Qwen3-SmVL

# LLM Training Framework

## llm-foundry
https://github.com/mosaicml/llm-foundry

<img width="1106" height="454" alt="image" src="https://github.com/user-attachments/assets/0e536b60-2ce4-4e25-b025-7e0ca141208a" />


## LLaMA-Factory
https://github.com/hiyouga/LLaMA-Factory
<img width="1400" height="400" alt="image" src="https://github.com/user-attachments/assets/d07b207b-7882-4de7-a376-f5d155d244a8" />

## ms-swift
https://github.com/modelscope/ms-swift
<img width="2048" height="544" alt="image" src="https://github.com/user-attachments/assets/2533eade-9d7f-4253-b8a9-62baec8060b4" />

## unsloth

<img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/dbdd0369-fee3-4a23-ac7b-a56e836c6be2" />

https://unsloth.ai/

## trl
https://huggingface.co/docs/trl/index

<img width="363" height="90" alt="image" src="https://github.com/user-attachments/assets/3192f1c3-fa76-4c98-917a-ba69563799e8" />

Fine-Tuning a Vision Language Model (Qwen2-VL-7B) with the Hugging Face Ecosystem (TRL)

https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl


https://huggingface.co/blog/vllm-colocate

## transformers
<img width="2240" height="780" alt="image" src="https://github.com/user-attachments/assets/75f1917b-3f70-4bb5-9f31-89d5648c5cbc" />

https://github.com/huggingface/transformers

https://huggingface.co/blog/smollm3

https://huggingface.co/blog/smolvlm2

https://github.com/huggingface/smollm

https://github.com/merveenoyan/smol-vision

# RAG


# LLM Inference Framework

## Vllm

## ollama

## https://github.com/NVIDIA/TensorRT-LLM

## optimum onnx
https://github.com/huggingface/optimum


# MCP

## FastAPI-MCP
https://github.com/tadata-org/fastapi_mcp

About
Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!

fastapi-mcp.tadata.com/

## Windows-MCP

## Pixelle-MCP

An Open-Source Multimodal AIGC Solution based on ComfyUI + MCP + LLM https://pixelle.ai
https://github.com/AIDC-AI/Pixelle-MCP

## Model compression toolkit

https://github.com/Tencent/AngelSlim



## LLM Cache

https://lmcache.ai/

Accelerating the Future of AI, One Cache at a Time

Supercharge Your LLM with the Fastest KV Cache Layer 

https://github.com/LMCache/LMCache

<img width="3630" height="1480" alt="image" src="https://github.com/user-attachments/assets/1a984910-5c1d-49dd-bccf-69f35f3cab77" />

https://github.com/LMCache/demo/tree/master/demo4-compare-with-vllm

<img width="1696" height="601" alt="image" src="https://github.com/user-attachments/assets/14ee6a14-d0c0-417a-b05b-5d743b1579c4" />



# World Model

## V-JPPA
https://ai.meta.com/vjepa/

https://github.com/facebookresearch/vjepa2


INTRODUCING V-JEPA 2

A self-supervised foundation world model Video Joint Embedding Predictive Architecture 2 (V-JEPA 2) is the first world model trained on video that achieves state-of-the-art visual understanding and prediction, enabling zero-shot robot control in new environments.


https://learnopencv.com/v-jepa-2-meta-world-model-robotics-guide/

<img width="1739" height="703" alt="image" src="https://github.com/user-attachments/assets/8ba31f21-82bf-4396-864d-54029c5f69dc" />


World Modeling with Probabilistic Structure Integration
https://arxiv.org/pdf/2509.09737v1

<img width="1199" height="1140" alt="image" src="https://github.com/user-attachments/assets/d8330feb-0aad-446f-ab3d-ecaee192f010" />

Introduction to World Models: V-JEPA 2 ï¼šhttps://www.youtube.com/watch?v=LY_J9-7BjOc


<img width="2386" height="1293" alt="image" src="https://github.com/user-attachments/assets/23caff98-2e16-41eb-9ea5-6763a0dfae4f" />


https://video-zero-shot.github.io/

Video models are zero-shot learners and reasoners


<img width="1830" height="651" alt="image" src="https://github.com/user-attachments/assets/3b388e1a-1fc3-469e-82fe-c4ef9be43f16" />

## Genie 3

## Skywork Matrix-Game 2.0

# Model on Device

## Gemma-3-270M

## Liquid AI

https://www.liquid.ai/

https://huggingface.co/LiquidAI/LFM2-VL-1.6B

Introducing LFM2: The Fastest On-Device Foundation Models on the Market

<img width="2391" height="1274" alt="image" src="https://github.com/user-attachments/assets/7ceebe08-5669-4aba-a7ad-d5e6497731dd" />




big vision model

https://www.hfuu.edu.cn/CVPR/ce/98/c10439a118424/page.htm



